{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673de73c-3a58-4b75-bca2-9917309967d4",
   "metadata": {},
   "source": [
    "# From ANN to ONN\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2b6bd-2c74-4312-8e55-8d1ba9f5a4d4",
   "metadata": {},
   "source": [
    "Given a keras file (neural network) and calibration data, build and train the photonic equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce40f56-c6db-43a2-9680-2dfa307070e4",
   "metadata": {},
   "source": [
    "## Read calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c279cf-0598-49ec-9570-0b6cd8fbf287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 15:34:28.761283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/pierrick/program/lzma/:/home/pierrick/program/python3_photon//install/Python-3.8.12/lib/:\n",
      "2023-02-13 15:34:28.761358: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-13 15:34:29.698682: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/pierrick/program/lzma/:/home/pierrick/program/python3_photon//install/Python-3.8.12/lib/:\n",
      "2023-02-13 15:34:29.698805: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/pierrick/program/lzma/:/home/pierrick/program/python3_photon//install/Python-3.8.12/lib/:\n",
      "2023-02-13 15:34:29.698817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 256)\n",
      "(60000, 10)\n",
      "(10000, 256)\n",
      "(10000, 10)\n",
      "(100, 256)\n",
      "[7 2 4 9 1 6 6 7 4 5 9 8 5 1 9 0 8 8 7 1 8 5 8 5 5 5 6 1 0 6 3 8 4 1 0 3 6\n",
      " 3 7 2 3 6 4 6 9 5 7 5 6 6 0 7 9 0 2 1 6 9 5 2 1 3 6 8 2 4 9 0 3 0 4 4 4 6\n",
      " 9 3 7 8 7 1 7 5 8 1 3 2 0 6 9 4 1 1 9 2 8 6 3 7 8 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# synthetic data\n",
    "#calib_data=np.random.uniform(-1., +1., (100, 256))\n",
    "\n",
    "# realistic data\n",
    "N = 16\n",
    "nb_data = 100\n",
    "from ANN import get_db\n",
    "(train_X, train_Y), (test_X, test_Y) = get_db(\"MNIST\", N, shuffle=True)\n",
    "calib_data=train_X[:nb_data]\n",
    "\n",
    "\n",
    "print(calib_data.shape)\n",
    "print(np.argmax(train_Y[:nb_data],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58345a79-902e-42da-b952-12e0c5986831",
   "metadata": {},
   "source": [
    "## Read weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7a498-bdb2-4ac7-9ed6-fde52b40cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras_path=\"tmp/MNIST/model_0/\"\n",
    "ann = keras.models.load_model(keras_path)\n",
    "W1=ann.layers[1].get_weights()[0]\n",
    "W2=ann.layers[3].get_weights()[0]\n",
    "\n",
    "# free memory\n",
    "keras.backend.clear_session()\n",
    "del ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3129319-7748-468e-bb0e-f56076bc00f5",
   "metadata": {},
   "source": [
    "## Teacher predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f9b00-4fe0-4706-acbb-455855cd71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1=[]\n",
    "A=[]\n",
    "Y2=[]\n",
    "\n",
    "for x in calib_data:\n",
    "    y1=np.dot(W1.T, x.reshape(len(x),1))# reshape for giving it to matrix multiplication\n",
    "    a=np.maximum(y1,0)\n",
    "    y2=np.dot(W2.T, a)\n",
    "    \n",
    "    Y1.append(y1)\n",
    "    A.append(a)\n",
    "    Y2.append(y2)\n",
    "    \n",
    "Y1=np.array(Y1)\n",
    "A=np.array(A)\n",
    "Y2=np.array(Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32420b47-d97a-4a88-95c2-a7539a9f38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(Y2,axis=1).squeeze()==np.argmax(train_Y[:len(Y2)].squeeze(),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf63b49-1761-48e9-9460-0ff844cd289d",
   "metadata": {},
   "source": [
    "## Train the student to mimic the teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c966b57-47fe-4e6b-a134-386cf121c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TeacherStudent import TeacherStudent, TeacherMatrix\n",
    "\n",
    "# Teacher\n",
    "teacher_W1=TeacherMatrix(W1)\n",
    "teacher_W2=TeacherMatrix(W2)\n",
    "\n",
    "# Student\n",
    "import ONN\n",
    "hp={\"lr\":0.1, \"lr_decay\":10., \"layers\":[W1.shape[1]], \"pattern\": [\"triangle\"], \"col_layer_limit\": [8]}\n",
    "student_W1=ONN.ONN(hp,{},{\"epochs\":5, \"loss\":ONN.MSE, \"metrics\":ONN.MSE})\n",
    "\n",
    "hp={\"lr\":0.1, \"lr_decay\":10., \"layers\":[W2.shape[1]], \"pattern\": [\"triangle\"], \"col_layer_limit\": [8]}\n",
    "student_W2=ONN.ONN(hp,{},{\"epochs\":5, \"loss\":ONN.MSE, \"metrics\":ONN.MSE})\n",
    "\n",
    "\n",
    "teacher_student_W1=TeacherStudent(teacher_W1, student_W1)\n",
    "teacher_student_W2=TeacherStudent(teacher_W2, student_W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9cd5b-f417-4f9f-b21e-2c7e6d005eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_student_W1.fit(calib_data)\n",
    "teacher_student_W2.fit(A.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f0c69-1485-4199-aa3a-2fc7034cf901",
   "metadata": {},
   "source": [
    "### Prediction with the student models (ONN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3b86b-cbc6-487b-9e4f-7be732a7706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1_ONN=[]\n",
    "A_ONN=[]\n",
    "Y2_ONN=[]\n",
    "\n",
    "for x in calib_data:\n",
    "    y1=student_W1.predict(x)\n",
    "    a=np.maximum(x,0)\n",
    "    y2=student_W2.predict(x)\n",
    "    \n",
    "    Y1_ONN.append(y1)\n",
    "    A_ONN.append(a)\n",
    "    Y2_ONN.append(y2)\n",
    "Y1_ONN=np.array(Y1_ONN)\n",
    "A_ONN=np.array(A_ONN)\n",
    "Y2_ONN=np.array(Y2_ONN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6ec30-7a9d-4056-b853-62f79fdc8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(Y2_ONN,axis=1).squeeze()==np.argmax(train_Y[:len(Y2_ONN)].squeeze(),axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
