{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e65b9a9-42d5-471b-bf7c-8d55387ea24e",
   "metadata": {},
   "source": [
    "# Optical Neural Network with numpy/jax\n",
    "\n",
    "Simple example Y=WX\n",
    "![alt text](mzi_mesh.jpg \"Mesh type used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17670d-8cbe-4163-b746-ec4651a62b80",
   "metadata": {},
   "source": [
    "## Trainable photonic circuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f814ba-0810-43e6-b307-cd81238df0d4",
   "metadata": {},
   "source": [
    "###  Noise function according to AnalogVNN formula\n",
    "source: https://arxiv.org/pdf/2210.10048.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422cadb-33b2-4364-a739-a6159f89f765",
   "metadata": {},
   "source": [
    "![alt text](analogVNN.png \"AnalogVNN big picture figure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "852f1e8f-0b22-47b6-a0fb-59efbc874c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as np\n",
    "\n",
    "random_seed=1\n",
    "noisy=True\n",
    "\n",
    "def get_key():\n",
    "    global random_seed\n",
    "    random_seed+=1\n",
    "    return jax.random.PRNGKey(random_seed)\n",
    "\n",
    "def no_back(f): \n",
    "    \"\"\" Decorator to avoid backpropagation of the decorated function.\n",
    "    For example it is useful for \"round(x)\" \"\"\"\n",
    "    def decorated_f(x, *args):\n",
    "        # Create an exactly-zero expression with Sterbenz lemma that has\n",
    "        # an exactly-one gradient.\n",
    "        # URL : https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html\n",
    "        zero = x - jax.lax.stop_gradient(x)\n",
    "        return zero + jax.lax.stop_gradient(f(x, *args))\n",
    "    return decorated_f\n",
    "\n",
    "# Previous function implement noise AnalogVNN: https://arxiv.org/pdf/2210.10048.pdf\n",
    "def _rounding_with_thresh(g, r):\n",
    "    g_abs = np.abs(g)\n",
    "    g_floor = np.floor(g_abs)\n",
    "    g_ceil = np.ceil(g_abs)\n",
    "    prob_floor = 1. - np.abs(g_floor - g)\n",
    "    do_floor = np.array( r <= prob_floor, dtype=np.float32)\n",
    "    do_ceil = np.array( r > prob_floor, dtype=np.float32)\n",
    "    return do_floor * g_floor + do_ceil * g_ceil\n",
    "\n",
    "@no_back\n",
    "def precion_reduction(x, p):\n",
    "    \"\"\"warning precision=4 means 5 potential value:  {0,0.25,0.5,0.75,1}\n",
    "    substracting by 1 before calling it is maybe always required\"\"\"\n",
    "    r=0.5\n",
    "    g = x * p\n",
    "    f = np.sign(g) * _rounding_with_thresh(g, r) * (1. / p)\n",
    "    return f\n",
    "\n",
    "@no_back\n",
    "def stochastic_reduce_precision(x, p):\n",
    "    g = x * p\n",
    "    r=jax.random.uniform(shape=x.shape, key=get_key(), dtype=np.float32)\n",
    "    f = np.sign(g) * _rounding_with_thresh(g, r) * (1. / p)\n",
    "    return f\n",
    "\n",
    "def normalization(x):\n",
    "    return np.clip(x,-1.,+1.)\n",
    "\n",
    "@no_back\n",
    "def additive_noise(x, std):\n",
    "    noise=jax.random.normal(shape=x.shape, key=get_key(),dtype=np.float32) * std\n",
    "    return x+noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24860cde-29a2-4f3e-89ed-bc8fc7a3d976",
   "metadata": {},
   "source": [
    "### Circuits: MZI, column of MZI, mesh, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "52fe45c5-9abd-4a81-b9ac-7f76117143c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MZI(X, teta):\n",
    "    R = np.array([\n",
    "      [np.cos(teta), -np.sin(teta)],\n",
    "      [np.sin(teta), np.cos(teta)]\n",
    "    ])\n",
    "    out_vector=np.dot(R, X)\n",
    "    return out_vector\n",
    "\n",
    "def noisy_MZI(X, teta):\n",
    "    p_signal=2.**4\n",
    "    p_weights=2.**4\n",
    "    noise_signal=1e-3\n",
    "    noise_weights=1e-3\n",
    "    \n",
    "    X=additive_noise( precion_reduction( normalization(X) , p_signal ) , noise_signal)\n",
    "    teta=additive_noise( stochastic_reduce_precision( normalization(teta) , p_weights), noise_weights)\n",
    "    \n",
    "    y=MZI(X, teta)\n",
    "\n",
    "    y=precion_reduction( normalization( additive_noise(y, noise_signal ) ) , p_signal)\n",
    "\n",
    "    return y\n",
    "\n",
    "def MZI_col(X, nb_mzi, W):\n",
    "    \n",
    "    if noisy:\n",
    "        MZI_strat=noisy_MZI\n",
    "    else:\n",
    "        MZI_strat=MZI\n",
    "    \n",
    "    # Column type: odd or even ?\n",
    "    nb_pins=nb_mzi*2\n",
    "    if nb_pins==len(X):\n",
    "        start_pin_id=0\n",
    "    elif nb_pins+2==len(X):\n",
    "        start_pin_id=1\n",
    "    else:\n",
    "        raise ValueError(\"This mesh patern is not compatible with this input size and #MZIs\")\n",
    "\n",
    "    # pin them\n",
    "    layer_outputs=[]\n",
    "    if start_pin_id==1:\n",
    "        layer_outputs.append(np.array([X[0]]))\n",
    "    \n",
    "    for ID in range(0, nb_mzi):\n",
    "        # take input vector\n",
    "        first_pin_pos=2*ID+start_pin_id\n",
    "        second_pin_pos=first_pin_pos+1\n",
    "        local_inp = X[first_pin_pos:second_pin_pos+1]\n",
    "        \n",
    "        # compute the output vector\n",
    "        local_out=MZI_strat(local_inp, W[ID])\n",
    "        layer_outputs.append(local_out)\n",
    "    \n",
    "    if start_pin_id==1:\n",
    "        layer_outputs.append(np.array([X[-1]]))\n",
    "    \n",
    "    Y=np.concatenate(layer_outputs)\n",
    "    return Y\n",
    "\n",
    "def onn(X, nb_mzis, weights):\n",
    "    nb_layers=len(weights)\n",
    "\n",
    "    def recusive_layer_builder(id_layer=0):\n",
    "        if id_layer==nb_layers-1: # last layer. No dependency\n",
    "            input_shape=X.shape\n",
    "            y=MZI_col(X, nb_mzis[id_layer], weights[id_layer])\n",
    "        else:\n",
    "            y = recusive_layer_builder(id_layer + 1)\n",
    "            input_shape=y.shape\n",
    "            y=MZI_col(y, nb_mzis[id_layer], weights[id_layer])\n",
    "        return y\n",
    "\n",
    "    Y=recusive_layer_builder()\n",
    "    return Y\n",
    "\n",
    "def spec_mesh(cols, mzi_per_col): #e.g. 6,3->3,2,3,2,3,2\n",
    "    cols=n_comp\n",
    "    mzi_per_col=n_comp//2\n",
    "    nb_mzis=[]\n",
    "    for i in range(cols):\n",
    "        nb_mzis.append( mzi_per_col-i%2 )\n",
    "    return nb_mzis\n",
    "\n",
    "def glorot_init(nb_mzi):\n",
    "    weights=jax.random.normal(shape=(nb_mzi,), key=get_key(),dtype=np.float32) * np.sqrt(0.5)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5aac8e-ee3b-49cd-8449-8b3949a74f86",
   "metadata": {},
   "source": [
    "## Simple function learning: [0,1,] -> [1,0]\n",
    "Simple problem before solving harder problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fc785",
   "metadata": {},
   "source": [
    "#### Configuration of the training dataset and ONN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8582ab7-877e-412f-80eb-b3267fccaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([1, 0])\n",
    "Y=np.array([0, 1])\n",
    "nb_MZIs=(1,)\n",
    "W=[]\n",
    "for n in nb_MZIs:\n",
    "    W.append(glorot_init(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a57c6",
   "metadata": {},
   "source": [
    "#### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0039d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit(X, W):\n",
    "    return onn(X, nb_MZIs, W)\n",
    "    \n",
    "# Create the circuit with metric\n",
    "def circuit_to_opt(*args):\n",
    "    y_=circuit(*args)\n",
    "    loss=np.mean((Y-y_)**2)\n",
    "    return loss\n",
    "\n",
    "deriv_circuit_to_opt=jax.grad(circuit_to_opt, argnums=(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ccc5e-8994-46c6-ac7d-286f4e36e19a",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ce64b132-50d1-4b07-a6f8-9b1734479b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pred.: [ 1.     -0.1875]\n",
      "current loss: 1.2050781\n",
      "current loss: 0.6347656\n",
      "current loss: 0.26757812\n",
      "current loss: 0.17578125\n",
      "current loss: 0.17578125\n",
      "Final pred.: [0.5625 0.8125]\n"
     ]
    }
   ],
   "source": [
    "lr=0.5\n",
    "print(\"First pred.:\", circuit(X,W))\n",
    "for i in range(5):\n",
    "    \n",
    "    # forward phase\n",
    "    print(\"current loss:\", circuit_to_opt(X,W))\n",
    "\n",
    "    # backward phase\n",
    "    dW=deriv_circuit_to_opt(X,W)[0]\n",
    "\n",
    "    # Update using the gradient information\n",
    "    for i, dWi in enumerate(dW):\n",
    "        W[i] = W[i] - lr * dWi\n",
    "print(\"Final pred.:\", circuit(X,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7c163",
   "metadata": {},
   "source": [
    "## On-chip learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b10e40-5bff-4537-a81b-67e41ea7fdff",
   "metadata": {},
   "source": [
    "#### Information about on-chip learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4af6df-dfdf-4c5a-a995-76d53ccef186",
   "metadata": {},
   "source": [
    "* Stochastic Gradient Descent optimization (code below):\n",
    "    * Pros:\n",
    "        * Speed and Scalability when the dimensionality (#params) increase\n",
    "    * Cons: \n",
    "        * Above code need to be embedded on-chip\n",
    "        * Noisy gradient (E.g. MZI noise) -> catastrophic performance (E.g. > 0.001)\n",
    "* Other optimizer exists:\n",
    "    * Example:\n",
    "        * Forward gradient descent\n",
    "        * Simulated annealing\n",
    "        * ...\n",
    "    * Pros:\n",
    "        * Simpler to implement (no backpropagation)\n",
    "    * Cons: \n",
    "        * They do not scale well when the dimensionality increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bfa0bdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import jax\n",
      "from jax.numpy import *\n",
      "from jax._src import prng\n",
      "def f(b, c):\n",
      "    a = array([0, 1], dtype=int32)\n",
      "    d = jax.lax.dynamic_slice_in_dim(c, 0, (1,)[0], axis=0)\n",
      "    e = squeeze(array(d))\n",
      "    def local_f0(a, b, c):\n",
      "        d = array(a).astype(float32)\n",
      "        e = array([max(b)])\n",
      "        f = array([min(c)])\n",
      "        return f\n",
      "    f = local_f0(b, -1.0, 1.0)\n",
      "    g = f # stop grad\n",
      "    h = f - g\n",
      "    i = f * 16.0\n",
      "    def local_f1(a):\n",
      "        b = sign(a)\n",
      "        return b\n",
      "    j = local_f1(i)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/pierrick/PycharmProjects/JaxDecompiler/\") # <-- Your path here\n",
    "for module_name in [\"decompiler\", \"primitive_mapping\"]:\n",
    "    if module_name in sys.modules:\n",
    "        os.remove(sys.modules[module_name].__cached__)  # remove cached bytecode\n",
    "        del sys.modules[module_name]\n",
    "import decompiler # JaxDecompiler\n",
    "df, c= decompiler.python_jaxpr_python(deriv_circuit_to_opt, (X, W), is_python_returned=True)\n",
    "print(\"\\n\".join(c.split(\"\\n\")[:20])) # print the 20 first lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb341b51-6301-4784-affe-3bdb36403658",
   "metadata": {},
   "source": [
    "## MNIST classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8da22a-4d92-458f-9bec-36f58fca306b",
   "metadata": {},
   "source": [
    "Commonly used dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a2c43-4473-417b-abfc-7bfceeabc366",
   "metadata": {},
   "source": [
    "#### Read the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dd4ed0e8-e92f-43f3-a891-ff77d9d2854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as npo\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a74ba5-9264-4f63-88d1-bca3234c1be3",
   "metadata": {},
   "source": [
    "#### Preprocessing dataset \n",
    "Croping, interpolating, intensity scaling, reshaping, projection, shuffling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6489ca84-65ec-4fdf-9dce-6de8e2850b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA variance explained: 0.521593761795937\n",
      "(60000, 10)\n",
      "(60000, 10)\n",
      "(10000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Cropping\n",
    "train_X=train_X[:,4:24,4:24]\n",
    "test_X=test_X[:,4:24,4:24]\n",
    "\n",
    "# Interpolating\n",
    "from scipy.ndimage import zoom\n",
    "train_X= zoom(train_X,(1.,.5,.5),order=3)  #order = 3 for cubic interpolation\n",
    "test_X= zoom(test_X,(1.,.5,.5),order=3)\n",
    "\n",
    "# intensity scaling and flatting\n",
    "train_X=train_X.reshape((len(train_X),10*10))/255.\n",
    "test_X=test_X.reshape((len(test_X),10*10))/255.\n",
    "\n",
    "# projection\n",
    "n_comp=10 # variance explained is only 52%\n",
    "from sklearn.decomposition import PCA\n",
    "proj = PCA(n_components = n_comp)\n",
    "train_X = proj.fit_transform(train_X)\n",
    "test_X=proj.transform(test_X)\n",
    "print(f\"PCA variance explained: {sum(proj.explained_variance_ratio_)}\")\n",
    "\n",
    "# label processing into one-hot vector\n",
    "train_y2=npo.zeros((len(train_X),n_comp),dtype=float)\n",
    "test_y2=npo.zeros((len(test_X),n_comp), dtype=float)\n",
    "for i,v in enumerate(train_y):\n",
    "    train_y2[i][v]=1.\n",
    "\n",
    "for i,v in enumerate(test_y):\n",
    "    test_y2[i][v]=1.\n",
    "\n",
    "# shuffling\n",
    "ids=npo.array(range(len(train_X)))\n",
    "npo.random.shuffle(ids)\n",
    "train_X=train_X[ids]\n",
    "train_y2=train_y2[ids]\n",
    "\n",
    "ids=npo.array(range(len(test_X)))\n",
    "npo.random.shuffle(ids)\n",
    "test_X=test_X[ids]\n",
    "test_y2=test_y2[ids]\n",
    "\n",
    "# Dimension check\n",
    "print(train_X.shape)\n",
    "print(train_y2.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328adb35-82e3-477d-a659-719b56f2b738",
   "metadata": {},
   "source": [
    "#### Definition of the ONN (forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1e8b7e46-6431-4876-9152-24525147136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init weights\n",
    "nb_mzis=spec_mesh(10, 5)\n",
    "W=[]\n",
    "for n in nb_mzis:\n",
    "    W.append(glorot_init(n))\n",
    "\n",
    "# compilation of the onn\n",
    "def onn10(X, W):\n",
    "    return onn(X, nb_mzis, W)\n",
    "circuit=jax.jit(onn10) # JIT circuit is ~3300 times faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3f996-2b80-4ea6-ba00-1eccdc6cf927",
   "metadata": {},
   "source": [
    "#### Backward definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b7c3f47-068b-4dbe-b97a-68d279a71839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the circuit with metric\n",
    "def circuit_to_opt(*args):\n",
    "    y_pred=circuit(*(args[0], args[2])) #0:X, 2:W\n",
    "    y_expected=args[1] #1:Y\n",
    "    loss=np.mean((y_expected-y_pred)**2)\n",
    "    return loss\n",
    "\n",
    "deriv_circuit_to_opt=jax.grad(circuit_to_opt, argnums=(-1,))\n",
    "deriv_circuit_to_opt=jax.jit(deriv_circuit_to_opt) # JIT circuit is ~3300 times faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510814-d0f0-4ede-8905-2d72a96073f0",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dd839aaa-e3c1-4e6d-821b-56eac3719ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.3709\n",
      "accuracy:0.3736\n",
      "accuracy:0.3752\n",
      "accuracy:0.371\n",
      "accuracy:0.3676\n"
     ]
    }
   ],
   "source": [
    "random_seed=1\n",
    "lr=0.1\n",
    "\n",
    "for e in range(5): # for each epoch\n",
    "    \n",
    "    # Data shuffling\n",
    "    ids=npo.array(range(len(train_X)))\n",
    "    npo.random.shuffle(ids)\n",
    "    train_X=train_X[ids]\n",
    "    train_y2=train_y2[ids]\n",
    "    \n",
    "    # Training\n",
    "    for X,Y in zip(train_X, train_y2): # for each data sample\n",
    "        # backward phase\n",
    "        dW=deriv_circuit_to_opt(X, Y, W)[0]\n",
    "\n",
    "        # Update using the gradient information\n",
    "        for i, dWi in enumerate(dW):\n",
    "            W[i] = W[i] - lr * dWi\n",
    "\n",
    "    # Evaluation\n",
    "    nb_correct=0\n",
    "    for X,Y in zip(test_X, test_y2):        \n",
    "        y_pred=circuit(X, W)\n",
    "        nb_correct+=np.argmax(y_pred)==np.argmax(Y)\n",
    "    print(f\"accuracy:{float(nb_correct)/len(test_y2)}\")\n",
    "    lr/=10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3acefc8",
   "metadata": {},
   "source": [
    "#### Ensemble of ONN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62428aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model 0: 0.4347\n",
      "Accuracy of the model 1: 0.3755\n",
      "Accuracy of the model 2: 0.4251\n",
      "Accuracy of the model 3: 0.4344\n",
      "Accuracy of the model 4: 0.4163\n"
     ]
    }
   ],
   "source": [
    "nb_mzis=spec_mesh(10, 5)\n",
    "lr=0.1\n",
    "\n",
    "Ensemble_W=[]\n",
    "for ens in range(5):\n",
    "    \n",
    "    W2=[]\n",
    "    for n in nb_mzis:\n",
    "        W2.append(glorot_init(n))\n",
    "    \n",
    "    # Data shuffling\n",
    "    ids=npo.array(range(len(train_X)))\n",
    "    npo.random.shuffle(ids)\n",
    "    train_X=train_X[ids]\n",
    "    train_y2=train_y2[ids]\n",
    "    \n",
    "    #training\n",
    "    for X,Y in zip(train_X, train_y2): # for each data sample\n",
    "        # backward phase\n",
    "        dW=deriv_circuit_to_opt(X, Y, W2)[0]\n",
    "\n",
    "        # Update using the gradient information\n",
    "        for i, dWi in enumerate(dW):\n",
    "            W2[i] = W2[i] - lr * dWi\n",
    "\n",
    "    # Evaluation\n",
    "    nb_correct=0\n",
    "    for X,Y in zip(test_X, test_y2):        \n",
    "        y_pred=circuit(X, W2)\n",
    "        nb_correct+=np.argmax(y_pred)==np.argmax(Y)\n",
    "    print(f\"Accuracy of the model {ens}: {float(nb_correct)/len(test_y2)}\")\n",
    "\n",
    "    Ensemble_W.append(W2)\n",
    "    \n",
    "# Ensemble evaluation\n",
    "nb_correct=0\n",
    "for X,Y in zip(test_X, test_y2):        \n",
    "    y_pred=np.average(np.array([circuit(X, Wi) for Wi in Ensemble_W]),axis=0)\n",
    "    nb_correct+=np.argmax(y_pred)==np.argmax(Y)\n",
    "print(f\"Ensemble accuracy: {float(nb_correct)/len(test_y2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ed666-1031-48f5-a7ab-658cbfa1e5a1",
   "metadata": {},
   "source": [
    "Conclusion about the ensemble (with no noise):\n",
    "\n",
    "    * The ensemble is better than base ONN in it\n",
    "    * An ensemble of 5 base ONNs trained 1 epoch (67.56%) > 1 ONN trained 5 epochs (66.91%)  -> Better usage of computing ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa7c5c-b75a-4709-a640-a0ec46678837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
